---
title: "p8105_hw2_KAB2310"
author: "Kamiah Brown"
date: "2024-10-01"
output: github_document
---
## This is my submission for Homework 5.

#### Set up 
```{r}
library(tidyverse)
library(dplyr)
library(readxl)
library(broom)
```

## Problem 1
#### Function 
```{r}
bday_sim = function(n) {
  sample = sample(1:365, size = n, replace = TRUE)
duplicates = length(unique(sample)) < n
return(duplicates)
}

bday_sim(7)
```

```{r}
#### Running function 1000 times for each group size between 2 and 50
bday_sim = expand_grid(
    iter = 1:10000,
    n = 2:50) |>
  mutate(res = map_lgl(n, bday_sim)) |> 
  group_by(n) |> 
  summarize(
    prob = mean(res)
  )
```

#### Probability as a function (Shared Birthday) of group size
```{r}
bday_sim |>
  ggplot(aes(x = n, y = prob)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Probability of Shared Birthday vs Group",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()
```

### Comments
The plot demonstrates that as the group size increases, the probability of at least two people sharing a birthday rises quickly at first and then levels off as it approaches 1, highlighting a nonlinear relationship. While smaller groups show a rapid increase in the likelihood of shared birthdays, the rate of increases slows signficantly for larger group sizes (beyond 30), where the probability is approaching 1.

## Problem 2 
```{r}
n = 30
sigma = 5

sim_power = function(mu){
  
  tibble(
    x = rnorm(n, mu, sigma)) |> 
      summarize(
        tidy(t.test(x, mu = 0, conf.level = 0.95))) |> 
    select(estimate, p.value)
}

# Generate 5000 data sets from the model
sim_result = expand_grid(
  mu = 0:6,
  iter = 1:5000) |> 
  mutate(samp_res = map(mu, sim_power)) |> 
  unnest(samp_res)

```
#### Plot showing the proportion of times the null was rejected 
```{r}
sim_result |> 
  group_by(mu) |> 
  summarize(
    power = mean(p.value < 0.05)
  ) |> 
  ggplot(aes(x = mu, y = power)) +
  geom_point() +
  geom_line()
```

### Comments
When the effect size, mu, increases, the power of the test also increases. This means that the likelihood of correctly rejecting the null hypothesis rises as mu deviates further from 0. 


#### Plot showing the average estimate of mu and an overlay on the graph showing the average estimate of mu only in samples for which the null was rejected. 
```{r}
df1 = sim_result |> 
  group_by(mu) |> 
  summarize(avg = mean(estimate))  |> 
  mutate(category = "true")
```

```{r}
df2 = sim_result |> 
  filter(p.value < 0.05) |> 
  group_by(mu) |> 
  summarize(avg = mean(estimate)) |> 
  mutate(category = "rejected")

#Combining data frames
df <- rbind(df1, df2)

#Plot 2
ggplot(df, aes(x = mu, y = avg, color = category)) +
  geom_point() +  
  geom_line() +  
  labs(
    title = "Average Estimate by Mu",
    x = "Mu",
    y = "Average Estimate",
    color = "Category"
  ) +
  theme_minimal()
```

### Comments 
The sample average of 𝜇̂ across tests for which the null is rejected approximately equal to the true value of 𝜇because, when the sample is significantly different from the null distribution, we are more likely to reject the null hypothesis. However, the sample average of 𝜇 in such cases approximates the true value of 𝜇 more closely as the true value of 𝜇 increases.
 
# Problem 3
```{r}
#Read and created the city_state variable.
homicide_df = 
  read_csv("homicide-data.csv") |>
  mutate(city_state = str_c(city, state, sep = ", "))

head(homicide_df)
```
The raw data set have 12 variables and 52179 observations. The variables in the dataset includes uid, reported_date, victim_last, victim_first, victim_race, victim_age, victim_sex, city, state, lat, lon, disposition.

```{r}
homicide_sum <- homicide_df |>
  group_by(city_state) |>
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

head(homicide_sum)
```

#### Estimate the proportion of homicides that are unsolved
```{r}
baltimore_homi <- homicide_sum |>
  filter(city_state == "Baltimore, MD")
prop.test(n = pull(baltimore_homi,total_homicides), 
          x = pull(baltimore_homi,unsolved_homicides)) |>
  broom::tidy() |>
  select(estimate, conf.low, conf.high)
```

### Run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each
```{r}
cities_homi = homicide_sum |> 
  mutate(test_result = purrr::map2(unsolved_homicides, total_homicides, ~ {
    if (.y >= 30) {
      prop.test(x = .x, n = .y)
    } else {
      binom.test(x = .x, n = .y)
    }
  })) %>%
  mutate(test_result = purrr::map(test_result, broom::tidy)) %>%
  unnest(test_result) %>%
  select(city_state, estimate, conf.low, conf.high)

cities_homi
#I received an warning "Chi-squared approximation may be incorrect", so I modified using if/else statement and binom.test. 
```

#Plot that shows the estimates and CIs for each city
```{r}
cities_homi |>
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_bar(stat = "identity", fill = "lightblue") + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) + 
  coord_flip() +
  labs(
    x = "City",
    y = "Proportion of Unsolved Homicides",
    title = "Proportion of Unsolved Homicides by City with Confidence Intervals"
  ) +
  theme_minimal()
```






